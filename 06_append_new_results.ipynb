{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b168e3-5a44-4e4f-9267-05d339f1dd88",
   "metadata": {},
   "source": [
    "# Append new results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee9c83-69d5-4c9b-9471-4ed6687b5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ac5e6-3779-49cb-ad17-015b40aea24e",
   "metadata": {},
   "source": [
    "### Before appending the results, let's check that each folder contains the same unique file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa56e90-750b-4946-8c37-cd6e767d2dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a882b80-39ee-4c5c-b351-a60c04b82ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up folder prefixes \n",
    "prefix_1 = \"s3://dea-public-data-dev/projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/historic_data/\"\n",
    "prefix_2 = \"s3://dea-public-data-dev/projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/polygon_base_result_rearranged_cols/\"\n",
    "\n",
    "# list all the csvs in each folder \n",
    "files_1 = s3.glob(prefix_1 + \"*.csv\")\n",
    "files_2 = s3.glob(prefix_2 + \"*.csv\")\n",
    "\n",
    "# remove all the parts except the actual file name \n",
    "names_1 = set(f.split(\"/\")[-1] for f in files_1)\n",
    "names_2 = set(f.split(\"/\")[-1] for f in files_2)\n",
    "\n",
    "# compare the sets of file names \n",
    "only_in_1 = names_1 - names_2\n",
    "only_in_2 = names_2 - names_1\n",
    "common = names_1 & names_2\n",
    "\n",
    "print(f\"Files in folder 1: {len(names_1)}\")\n",
    "print(f\"Files in folder 2: {len(names_2)}\")\n",
    "print(f\"Files with matching names: {len(common)}\")\n",
    "\n",
    "if only_in_1:\n",
    "    print(f\"\\nFiles only in folder 1 ({len(only_in_1)}):\")\n",
    "    print(list(only_in_1)[:10])  \n",
    "\n",
    "if only_in_2:\n",
    "    print(f\"\\nFiles only in folder 2 ({len(only_in_2)}):\")\n",
    "    print(list(only_in_2)[:10])\n",
    "\n",
    "if not only_in_1 and not only_in_2:\n",
    "    print(\"\\n✅ All files match by name, no missing files in either folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0f1a6-7a54-4ffe-a57f-8f01434d95d4",
   "metadata": {},
   "source": [
    "### Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd1d64-f59c-47b5-b0b7-9ea092f6991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_aws_access_key_id=\"\"\n",
    "temp_aws_secret_access_key=\"\"\n",
    "temp_aws_session_token=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51b56b-d46b-4b73-879e-56f3f278f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', aws_access_key_id=temp_aws_access_key_id,\n",
    "                      aws_secret_access_key=temp_aws_secret_access_key, \n",
    "                 aws_session_token=temp_aws_session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c97d0c-64ca-471f-ad8b-f0c474659f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dea-public-data-dev'\n",
    "src_prefix_1 = 's3://dea-public-data-dev/projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/historic_data/'\n",
    "src_prefix_2 = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/polygon_base_result_rearranged_cols/'\n",
    "out_prefix = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/merged'\n",
    "max_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32605ab4-7f9f-4951-98ca-c938e4c4591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the filenames from an s3 folder \n",
    "def list_all_keys(prefix):\n",
    "    keys = []\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        kwargs = {'Bucket': bucket, 'Prefix': prefix}\n",
    "        if continuation_token:\n",
    "            kwargs['ContinuationToken'] = continuation_token\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "        contents = response.get('Contents', [])\n",
    "        keys.extend([obj['Key'].replace(prefix, '') for obj in contents])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "# merges two csv with the same name, removes duplicate rows (if they are exactly the same - doesn't address time problem), \n",
    "# and uploads results\n",
    "def process_file(file_key):\n",
    "    try:\n",
    "        key1 = src_prefix_1 + file_key\n",
    "        key2 = src_prefix_2 + file_key\n",
    "        out_key = out_prefix + file_key\n",
    "\n",
    "        obj1 = s3.get_object(Bucket=bucket, Key=key1)\n",
    "        obj2 = s3.get_object(Bucket=bucket, Key=key2)\n",
    "\n",
    "        df1 = pd.read_csv(obj1['Body'])\n",
    "        df2 = pd.read_csv(obj2['Body'])\n",
    "\n",
    "        combined_df = pd.concat([df1, df2], ignore_index=True).drop_duplicates()\n",
    "\n",
    "        csv_buffer = StringIO()\n",
    "        combined_df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "        s3.put_object(Bucket=bucket, Key=out_key, Body=csv_buffer.getvalue())\n",
    "\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"Error processing {file_key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a264c9-8281-4312-a642-1ada738ca242",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_keys = list_all_keys(src_prefix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6b2d7-8a0f-4483-857d-26abf5eb1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c64ed3-f915-438f-a53f-d2082a1832e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    list(tqdm(executor.map(process_file, file_keys), total=len(file_keys), desc=\"Processing files\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1d83c6-2031-479a-9cb5-cfd066af0160",
   "metadata": {},
   "source": [
    "### Quick count check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d2aad-ff4d-4345-8584-6f66754a2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dea-public-data-dev'\n",
    "prefix = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/merged'\n",
    "\n",
    "# count files with pagination\n",
    "file_count = 0\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "# count initial batch\n",
    "file_count += len(response.get('Contents', []))\n",
    "\n",
    "# keep paginating if needed\n",
    "while response.get('IsTruncated'):\n",
    "    continuation_token = response.get('NextContinuationToken')\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "    file_count += len(response.get('Contents', []))\n",
    "\n",
    "print(f\"Total number of files: {file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef662a-cd10-4a77-826b-1e7aaf053a23",
   "metadata": {},
   "source": [
    "if count has an extra file its probably a folder marker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e984f4-6747-4615-b7d2-6bbc19871748",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "bucket = 'dea-public-data-dev'\n",
    "folder = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/merged/' \n",
    "max_workers = 40  \n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# helper list all keys under the prefix\n",
    "def list_all_keys(prefix):\n",
    "    keys = []\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        kwargs = {'Bucket': bucket, 'Prefix': prefix}\n",
    "        if continuation_token:\n",
    "            kwargs['ContinuationToken'] = continuation_token\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "        contents = response.get('Contents', [])\n",
    "        keys.extend([obj['Key'].replace(prefix, '') for obj in contents])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "# function to get row count for a single file\n",
    "def get_row_count(file_key):\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        return (file_key, len(df))  \n",
    "    except Exception as e:\n",
    "        return (file_key, None)  \n",
    "\n",
    "# list all files in the folder\n",
    "file_keys = list_all_keys(folder)\n",
    "\n",
    "# create a progress bar for parallel tasks\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(get_row_count, f\"{folder}{file_key}\"): file_key for file_key in file_keys}\n",
    "    file_row_counts = {}\n",
    "\n",
    "    # process the files in parallel and update the file_row_counts dict\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "        file_key, row_count = future.result()\n",
    "        if row_count is not None:\n",
    "            file_row_counts[file_key] = row_count\n",
    "\n",
    "df_counts = pd.DataFrame(file_row_counts.items(), columns=[\"File\", \"Row Count\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000ad62-73ac-47d0-b61e-5e6c8750e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of row counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_counts[\"Row Count\"], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Row Counts per File\")\n",
    "plt.xlabel(\"Row Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# print the top 10 and bottom 10 files by row count\n",
    "top_10 = df_counts.nlargest(10, \"Row Count\")\n",
    "bottom_10 = df_counts.nsmallest(10, \"Row Count\")\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(\"\\nTop 10 files with the most rows:\")\n",
    "    for _, row in top_10.iterrows():\n",
    "        print(f\"{row['File']} — {row['Row Count']} rows\")\n",
    "\n",
    "    print(\"\\nBottom 10 files with the least rows:\")\n",
    "    for _, row in bottom_10.iterrows():\n",
    "        print(f\"{row['File']} — {row['Row Count']} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24892b47-b577-4750-8000-2f1cf8a32126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "bucket = 'dea-public-data-dev'\n",
    "folder = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/historic_data/' \n",
    "max_workers = 40 \n",
    "\n",
    "# S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# lelper to list all keys under the prefix\n",
    "def list_all_keys(prefix):\n",
    "    keys = []\n",
    "    continuation_token = None\n",
    "    while True:\n",
    "        kwargs = {'Bucket': bucket, 'Prefix': prefix}\n",
    "        if continuation_token:\n",
    "            kwargs['ContinuationToken'] = continuation_token\n",
    "        response = s3.list_objects_v2(**kwargs)\n",
    "        contents = response.get('Contents', [])\n",
    "        keys.extend([obj['Key'].replace(prefix, '') for obj in contents])\n",
    "        if response.get('IsTruncated'):\n",
    "            continuation_token = response['NextContinuationToken']\n",
    "        else:\n",
    "            break\n",
    "    return keys\n",
    "\n",
    "# function to get row count for a single file\n",
    "def get_row_count(file_key):\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        return (file_key, len(df))  \n",
    "    except Exception as e:\n",
    "        return (file_key, None) \n",
    "\n",
    "# list all files in the folder\n",
    "file_keys = list_all_keys(folder)\n",
    "\n",
    "# create a progress bar for parallel tasks\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = {executor.submit(get_row_count, f\"{folder}{file_key}\"): file_key for file_key in file_keys}\n",
    "    file_row_counts = {}\n",
    "\n",
    "    # process the files in parallel and update the file_row_counts dict\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "        file_key, row_count = future.result()\n",
    "        if row_count is not None:\n",
    "            file_row_counts[file_key] = row_count\n",
    "\n",
    "df_counts = pd.DataFrame(file_row_counts.items(), columns=[\"File\", \"Row Count\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdffab0-11f0-4bf4-943e-ea7488fe5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of row counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_counts[\"Row Count\"], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Row Counts per File\")\n",
    "plt.xlabel(\"Row Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# print the top 10 and bottom 10 files by row count\n",
    "top_10 = df_counts.nlargest(10, \"Row Count\")\n",
    "bottom_10 = df_counts.nsmallest(10, \"Row Count\")\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(\"\\nTop 10 files with the most rows:\")\n",
    "    for _, row in top_10.iterrows():\n",
    "        print(f\"{row['File']} — {row['Row Count']} rows\")\n",
    "\n",
    "    print(\"\\nBottom 10 files with the least rows:\")\n",
    "    for _, row in bottom_10.iterrows():\n",
    "        print(f\"{row['File']} — {row['Row Count']} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96235e0-6a14-4e9a-85be-0e618426d405",
   "metadata": {},
   "source": [
    "### Check weird polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383739af-0e4e-43e2-ba9b-fd83b48b67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# S3 path config\n",
    "bucket = \"dea-public-data-dev\"\n",
    "base_path_old = \"projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/historic_data\"\n",
    "base_path_new = \"projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5b86a-1054-4838-8b50-ce186ffc6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of file IDs to check - from notebook 01\n",
    "file_ids = [\n",
    "    \"r1dvcwur6\", \"r1dyq4xpv\", \"r1dwnvrtu\", \"r1dwpvmtd\", \"r1dy0jxgk\", \"r1dy632dn\", \"r1dy6qbkv\",\n",
    "    \"r1epf61zy\", \"r1epsb3cp\", \"r1g4r71z2\", \"r1g4rk8t7\", \"r1g4r752g\", \"r1g4rxk1d\", \"r1g4xbxzg\",\n",
    "    \"r1g6811c0\", \"r1g727wce\", \"r1g72sjn5\", \"r1g72u997\", \"r1g72y2sw\", \"r1g789b0p\", \"r1g78chp1\",\n",
    "    \"r1gm39sc6\", \"r1gm3egu8\", \"r1gmd52n0\", \"r1gmf6x3e\", \"r1gmfknh5\", \"r1gmfqk8y\", \"r1gtbsgtq\",\n",
    "    \"r1gtbstzm\", \"r1gtbveyx\", \"r1gtbtvze\", \"r1gtbtw7g\", \"r1gtbty9h\", \"r1gtbwyy1\", \"r1gtbxjr6\",\n",
    "    \"r1gtbxur1\", \"r1gtbz31s\", \"r1gtbz8v7\", \"r1gufs07z\", \"r1guft7je\", \"r1gujdg6b\", \"r1gunp9ws\",\n",
    "    \"r1guxnzz8\", \"r1guxpn0j\", \"r1gw03p0p\", \"r1gw03qzq\", \"r1gw05vw4\", \"r1gw06cdz\", \"r1gw08kde\",\n",
    "    \"r1gw0d3mc\", \"r1gw0d657\", \"r1gw0dfey\", \"r1gw0dmgm\", \"r1gw0dsd7\", \"r1gw0hecw\", \"r1uj429nj\",\n",
    "    \"r1uj4hpre\", \"r1uj4r4u1\", \"r1uj685nd\", \"r1uj7rxjp\", \"r1uje2jhr\", \"r1x56qu1x\", \"r1x59b7h2\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180119b0-602c-4250-a676-2134a3905f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(file_id):\n",
    "    key_old = f\"{base_path_old}/{file_id}.csv\"\n",
    "    key_new = f\"{base_path_new}/{file_id}.csv\"\n",
    "\n",
    "    try:\n",
    "        df_old = load_csv_from_s3(bucket, key_old)\n",
    "        df_new = load_csv_from_s3(bucket, key_new)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error loading {file_id}: {e}\")\n",
    "        return\n",
    "\n",
    "    issues_found = False\n",
    "\n",
    "    # 1. no duplicate rows\n",
    "    dups_old = df_old.duplicated().sum()\n",
    "    dups_new = df_new.duplicated().sum()\n",
    "    if dups_old > 0 or dups_new > 0:\n",
    "        print(f\"\\n Sanity check for file ID: {file_id}\")\n",
    "        print(f\"⚠️ Duplicates found - Old: {dups_old}, New: {dups_new}\")\n",
    "        issues_found = True\n",
    "\n",
    "    # 2. column match\n",
    "    if list(df_old.columns) != list(df_new.columns):\n",
    "        if not issues_found:\n",
    "            print(f\"\\n Sanity check for file ID: {file_id}\")\n",
    "        print(\"⚠️ Column mismatch!\")\n",
    "        print(\"Old columns:\", df_old.columns.tolist())\n",
    "        print(\"New columns:\", df_new.columns.tolist())\n",
    "        issues_found = True\n",
    "\n",
    "    # 3. dates in order\n",
    "    if 'date' in df_old.columns and 'date' in df_new.columns:\n",
    "        df_old['date'] = pd.to_datetime(df_old['date'], errors='coerce')\n",
    "        df_new['date'] = pd.to_datetime(df_new['date'], errors='coerce')\n",
    "        if not df_old['date'].is_monotonic_increasing or not df_new['date'].is_monotonic_increasing:\n",
    "            if not issues_found:\n",
    "                print(f\"\\n Sanity check for file ID: {file_id}\")\n",
    "            print(\"⚠️ Dates not sorted!\")\n",
    "            print(\"Date sorted - Old:\", df_old['date'].is_monotonic_increasing, \n",
    "                  \"New:\", df_new['date'].is_monotonic_increasing)\n",
    "            issues_found = True\n",
    "\n",
    "    # 4. new file has more rows\n",
    "    rows_old = len(df_old)\n",
    "    rows_new = len(df_new)\n",
    "    if rows_new <= rows_old:\n",
    "        if not issues_found:\n",
    "            print(f\"\\n Sanity check for file ID: {file_id}\")\n",
    "        print(f\"⚠️ New file does not have more rows. Old: {rows_old}, New: {rows_new}\")\n",
    "        issues_found = True\n",
    "\n",
    "    # 5. row count difference is reasonable\n",
    "    if abs(rows_new - rows_old) > 50:\n",
    "        if not issues_found:\n",
    "            print(f\"\\n Sanity check for file ID: {file_id}\")\n",
    "        print(f\"⚠️ Row count difference too large! Old: {rows_old}, New: {rows_new}\")\n",
    "        issues_found = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28983c2-21e9-45e5-a15a-e9c7b3f75192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all file IDs\n",
    "for file_id in file_ids:\n",
    "    sanity_check(file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7884d6-0fe6-49c5-900e-6b654c403de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f98f8-54ed-478f-a147-14c2d72678e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784caca3-f4c2-4f26-9e7f-e106e183fcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2917f9b-ad4e-463c-8aeb-14213bc98349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94741b40-6c58-4edb-94dd-184e2c7854bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616cba-acfc-40d5-ac36-33792c10babc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
