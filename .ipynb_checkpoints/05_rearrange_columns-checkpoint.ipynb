{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92df511e-c51f-4311-affd-11c449d8670f",
   "metadata": {},
   "source": [
    "# Rearrange the columns of conflux outputs\n",
    "\n",
    "Conflux outputs the columns weird order. This will reorder the columns to 'feature_id', 'water', 'wet', 'bs', 'pv', 'npv', 'pc_missing', 'date', and remove 'norm_pv', 'norm_npv', 'norm_bs'. This is specific to the ANAE MDB data request. \n",
    "\n",
    "TO DO: add concurrency for next time (see append_new_results.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5c916-0caa-4c5b-807f-4238a734d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd1d64-f59c-47b5-b0b7-9ea092f6991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_aws_access_key_id=\"\"\n",
    "temp_aws_secret_access_key=\"\"\n",
    "temp_aws_session_token=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51b56b-d46b-4b73-879e-56f3f278f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', aws_access_key_id=temp_aws_access_key_id,\n",
    "                      aws_secret_access_key=temp_aws_secret_access_key, \n",
    "                 aws_session_token=temp_aws_session_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82f6f4-97b0-4e17-9355-d99f5285df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dea-public-data-dev'\n",
    "source_prefix = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/polygon_base_result/'\n",
    "destination_prefix = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/polygon_base_result_rearranged_cols/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4729788-ce71-40f3-a446-82eb78840a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column order\n",
    "new_column_order = [\n",
    "    'feature_id', 'water', 'wet', 'bs', 'pv', 'npv', 'pc_missing',\n",
    "    'date'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0943084-78e9-4628-aff0-f7a158c58133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this when you have more than 999 csv's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb8eaa-47bf-496f-8170-ffc9400387df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list csvs with pagination\n",
    "csv_files = []\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=source_prefix)\n",
    "\n",
    "# fetch files\n",
    "csv_files.extend([obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')])\n",
    "\n",
    "# check for more files if the result is truncated\n",
    "while response.get('IsTruncated'):  \n",
    "    # Continue with the next page of results\n",
    "    continuation_token = response.get('NextContinuationToken')\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=source_prefix, ContinuationToken=continuation_token)\n",
    "    csv_files.extend([obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')])\n",
    "\n",
    "# process CSVs\n",
    "for key in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    # download CSV content\n",
    "    csv_obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = csv_obj['Body'].read().decode('utf-8')\n",
    "\n",
    "    # read CSV into DataFrame\n",
    "    df = pd.read_csv(StringIO(body))\n",
    "\n",
    "    # drop the unwanted columns\n",
    "    df = df.drop(columns=['norm_pv', 'norm_npv', 'norm_bs'], errors='ignore')\n",
    "\n",
    "    # reorder columns\n",
    "    df = df[new_column_order]\n",
    "\n",
    "    # save to new S3 path\n",
    "    new_key = key.replace(source_prefix, destination_prefix)\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    s3.put_object(Bucket=bucket, Key=new_key, Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9621d8-d99d-4ed5-abc8-2c4d70a4104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use if you have less than 999 csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453187ec-503a-4a62-850e-abc31ecc44da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list csvs \n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=source_prefix)\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "\n",
    "for key in tqdm(csv_files, desc=\"Processing CSV files\"):\n",
    "    # download CSV content\n",
    "    csv_obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    body = csv_obj['Body'].read().decode('utf-8')\n",
    "\n",
    "    # read CSV into DataFrame\n",
    "    df = pd.read_csv(StringIO(body))\n",
    "\n",
    "    # Reorder columns\n",
    "    df = df[new_column_order]\n",
    "\n",
    "    # save to new S3 path\n",
    "    new_key = key.replace(source_prefix, destination_prefix)\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    s3.put_object(Bucket=bucket, Key=new_key, Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57625c4a-f842-4486-98c1-1db671256bfc",
   "metadata": {},
   "source": [
    "### Quick count check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd98ef-928e-40fa-a71e-d2271fa6ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'dea-public-data-dev'\n",
    "prefix = 'projects/WIT/MDBA_ANAE_WIT_MH_18_04_2025/polygon_base_result_rearranged_cols/'\n",
    "\n",
    "# Count files with pagination\n",
    "file_count = 0\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "# Count initial batch\n",
    "file_count += len(response.get('Contents', []))\n",
    "\n",
    "# Keep paginating if needed\n",
    "while response.get('IsTruncated'):\n",
    "    continuation_token = response.get('NextContinuationToken')\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, ContinuationToken=continuation_token)\n",
    "    file_count += len(response.get('Contents', []))\n",
    "\n",
    "print(f\"Total number of files: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf42db-02fb-4a0a-868b-58fc60d36178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1031c043-80a8-4046-8701-98e155cab279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
